{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook I am going to implement a Batch Gradient Descent with Early Stopping using Softmax Regression model <b>without Scikit-Learn</b> (or at least try to do it :D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "I will use Iris dataset from sklearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 2:]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this dataset is sorted. If we would be using SGD there will be a need to shuffle it, but as we use BGD it may remain as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression using BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.51169627e-06 1.23942221e-06]\n",
      "[[2.18765223 2.24309829]\n",
      " [0.59692038 0.65236644]]\n",
      "[[ 0.11771323 -0.11642048]\n",
      " [ 0.99618503  0.76205132]]\n"
     ]
    }
   ],
   "source": [
    "theta1 = np.random.randn(2,1)\n",
    "theta2 = np.random.randn(2,1)\n",
    "theta3 = np.random.randn(2,1)\n",
    "\n",
    "m = X.shape[0]\n",
    "epochs = 100\n",
    "eta = 0.1\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    s1 = x.T.dot(theta1)\n",
    "    s2 = x.T.dot(theta2)\n",
    "    s3 = x.T.dot(theta3)\n",
    "    \n",
    "    s = np.c_[s1, s2, s3].ravel()\n",
    "    # print(s)\n",
    "    \n",
    "    p1 = np.exp(s1) / sum(np.exp(s))\n",
    "    p2 = np.exp(s2) / sum(np.exp(s))\n",
    "    p3 = np.exp(s3) / sum(np.exp(s))\n",
    "    \n",
    "    # print(p1, p2, p3)\n",
    "    \n",
    "    y_up = np.argmax(s)\n",
    "    # print(y_up)\n",
    "    \n",
    "    yk1, yk2, yk3 = 0, 0, 0\n",
    "    \n",
    "    if y[i] == 0:\n",
    "        yk1 = 1\n",
    "    elif y[i] == 1:\n",
    "        yk2 = 1\n",
    "    elif y[i] == 2:\n",
    "        yk3 = 1\n",
    "    \n",
    "    grad1 = 1/m * sum(p1 - yk1)*x\n",
    "    grad2 = 1/m * sum(p2 - yk2)*x\n",
    "    grad3 = 1/m * sum(p3 - yk3)*x\n",
    "    \n",
    "    theta1 = theta1 - eta*grad1\n",
    "    theta2 = theta2 - eta*grad2\n",
    "    theta3 = theta3 - eta*grad3\n",
    "       \n",
    "\n",
    "print(eta*grad1)\n",
    "print(theta2)\n",
    "print(theta3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper Solution\n",
    "I had a lot of trouble with this exercise so I decided to figure it out with the help of A. Geron github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 2:]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the bias term for every instance (x0 = 1). Also let's set random seed to make output of this solution reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "\n",
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(test_ratio * total_size)\n",
    "validation_size = int(validation_ratio * total_size)\n",
    "train_size = total_size - validation_size - test_size\n",
    "\n",
    "# Shuffling the dataset\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a one-hot vector of class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 1 1 0 1 1 1 0]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    \n",
    "    Y_one_hot = np.zeros((m,n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    \n",
    "    return Y_one_hot\n",
    "\n",
    "print(y_train[:10])\n",
    "print(to_one_hot(y_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_valid_one_hot = to_one_hot(y_valid)\n",
    "Y_test_one_hot = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the sotfmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    return exps/exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
    "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainig the Sofmtax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.446205811872683\n",
      "500 0.8350062641405651\n",
      "1000 0.6878801447192402\n",
      "1500 0.6012379137693313\n",
      "2000 0.5444496861981872\n",
      "2500 0.5038530181431525\n",
      "3000 0.4729228972192248\n",
      "3500 0.44824244188957774\n",
      "4000 0.4278651093928793\n",
      "4500 0.4106007142918712\n",
      "5000 0.3956780375390374\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "\n",
    "epsilon = 1e-7 # to avoid nans while computing loss\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "        print(iteration, loss)\n",
    "    \n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    \n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.32094157, -0.6501102 , -2.99979416],\n",
       "       [-1.1718465 ,  0.11706172,  0.10507543],\n",
       "       [-0.70224261, -0.09527802,  1.4786383 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "acc_score = np.mean(y_predict == y_valid)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some l2 regularization and increase the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.629842469083912\n",
      "500 0.5339667976629506\n",
      "1000 0.503640075014894\n",
      "1500 0.4946891059460321\n",
      "2000 0.4912968418075477\n",
      "2500 0.48989924700933296\n",
      "3000 0.4892990598451198\n",
      "3500 0.489035124439786\n",
      "4000 0.4889173621830817\n",
      "4500 0.4888643337449302\n",
      "5000 0.4888403120738818\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "\n",
    "epsilon = 1e-7 # to avoid nans while computing loss\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "        l2_loss = 1/2 * np.sum(np.square(Theta[1:])) #dont regularize first element since it coreesponds to bias term\n",
    "        loss = xentropy_loss + alpha*l2_loss\n",
    "        print(iteration, loss)\n",
    "    \n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha*Theta[1:]]\n",
    "    \n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.7096017363419875\n",
      "500 0.5739711987633519\n",
      "1000 0.5435638529109127\n",
      "1500 0.5355752782580262\n",
      "2000 0.5331959249285544\n",
      "2500 0.5325946767399383\n",
      "2765 0.5325460966791898\n",
      "2766 0.5325460971327977 early stopping!\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 \n",
    "n_iterations = 15001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1  # regularization hyperparameter\n",
    "best_loss = np.inf\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha*Theta[1:]]\n",
    "    \n",
    "    Theta = Theta - eta * gradients\n",
    "    \n",
    "    logits = X_valid.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:])) #dont regularize first element since it coreesponds to bias term\n",
    "    loss = xentropy_loss + alpha*l2_loss\n",
    "        \n",
    "    if iteration % 500 == 0:\n",
    "       print(iteration, loss)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration-1, best_loss)\n",
    "        print(iteration, loss, \"early stopping!\")\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the models accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_test.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7fdb54b42d75f7dc4bc7211c14a537c22f55fe700ff72c130752bbe38f9a9cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
